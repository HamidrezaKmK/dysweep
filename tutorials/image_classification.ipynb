{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Tutorial\n",
    "\n",
    "This is a friendly tutorial to cover a basic image classification task. We will leverage the capabilities of `dysweep` to run different experiments on all the different configurations of the model design, as well as the dataset.\n",
    "\n",
    "First run the following piece of code to set the root directory of the project as the parent directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parent directory as the working directory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# remove the current directory from the path\n",
    "sys.path = sys.path[1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To get started, we should first imagine the entire pipeline of our experiment. In this simple case, we have to first download the datasets for classification (in this case CIFAR10), perform any preprocessing and transforms needed to create a dataloader. Furthermore, we will also pick a specific model designed for the task at hand and train it using an optimizer. \n",
    "\n",
    "Each of the different parts of the experiments have different hyperparameters and configurations. It is good to come up with a plan of how we want to structure our configurations beforehand. A simple sketch of the run configurations as well as the simple training loop itself is given below that logs all the results in `wandb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"image_classification\")\n",
    "\n",
    "# Configurations and hyper-parameters\n",
    "\n",
    "# Training configurations\n",
    "EPOCH_COUNT = 10\n",
    "LR = 0.001\n",
    "# Data configurations\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 64\n",
    "# Model configurations\n",
    "NUM_CLASSES = 10\n",
    "PRETRAINED_OR_NOT = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define transformations for the train set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Load the ResNet model\n",
    "model = resnet50(pretrained=PRETRAINED_OR_NOT, num_classes=NUM_CLASSES).to(device) \n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training function\n",
    "def train(epoch_count=EPOCH_COUNT):\n",
    "    model.train()\n",
    "    for epoch in range(epoch_count):  # 10 epochs\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            \n",
    "    # compute the test accuracy\n",
    "    correct = 0\n",
    "    for i, data in tqdm(enumerate(test_loader, 0)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        # get the maximum logit\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # check if the prediction is correct\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    wandb.log({\"accuracy\": correct / len(test_set)})\n",
    " \n",
    "try:   \n",
    "    train()\n",
    "finally:\n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic and Configurable using Dypy\n",
    "\n",
    "To illustrate how our configurations work, we will make the above code configurable down to some of the very basic components. Usually, there is no need for such detailed configuration setup; however, we do this to showcase some features of out pipeline:\n",
    "\n",
    "1. Outline some of the functionalities of the [DyPy](https://github.com/vahidzee/dypy) library.\n",
    "2. Illustrate that the configurations can be complex and hierarchical, but generic in a sense that you can manipulate everything down to the very last detail using a configuration file.\n",
    "\n",
    "Furthermore, we will define a YAML configuration file that will allow us to define the entire experiment, and that these experiments are tunable down to the very last details.\n",
    "\n",
    "```YAML\n",
    "data:\n",
    "    dataset_class: torchvision.datasets.CIFAR10\n",
    "    batch_size: 64\n",
    "    num_workers: 2\n",
    "    train_transforms:\n",
    "    -   class_path: torchvision.transforms.RandomHorizontalFlip\n",
    "    -   class_path: torchvision.transforms.RandomCrop\n",
    "        init_args:\n",
    "            size: 32\n",
    "            padding: 4\n",
    "    -   class_path: torchvision.transforms.ToTensor\n",
    "    -   class_path: torchvision.transforms.Normalize\n",
    "        init_args:\n",
    "            mean: [0.4914, 0.4822, 0.4465]\n",
    "            std: [0.2023, 0.1994, 0.2010]\n",
    "    test_transforms:\n",
    "    -   class_path: torchvision.transforms.ToTensor\n",
    "    -   class_path: torchvision.transforms.Normalize\n",
    "        init_args:\n",
    "            mean: [0.4914, 0.4822, 0.4465]\n",
    "            std: [0.2023, 0.1994, 0.2010]\n",
    "model:\n",
    "    class_path: torchvision.models.resnet50\n",
    "    init_args:\n",
    "        pretrained: False\n",
    "        num_classes: 10\n",
    "trainer:\n",
    "    epoch_count: 10\n",
    "    optimizer:\n",
    "        class_path: torch.optim.SGD\n",
    "        init_args:\n",
    "            lr: 0.001\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we assume `config` is the above YAML configuration, we can re-write the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import dypy as dy\n",
    "import yaml\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load conf.yaml into the dictionary config\n",
    "with open(\"conf.yaml\", 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "\n",
    "# Define transformations for the train set\n",
    "train_transform = transforms.Compose([\n",
    "    dy.eval(x['class_path'])(**(x['init_args'] if 'init_args' in x else {})) for \n",
    "        x in cfg['data']['train_transforms']\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "test_transform = transforms.Compose([\n",
    "    dy.eval(x['class_path'])(**(x['init_args'] if 'init_args' in x else {})) for \n",
    "        x in cfg['data']['test_transforms']\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = dy.eval(cfg['data']['dataset_class'])(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_set = dy.eval(cfg['data']['dataset_class'])(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=cfg['data']['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=cfg['data']['num_workers'],\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=cfg['data']['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=cfg['data']['num_workers'],\n",
    ")\n",
    "\n",
    "# Load the ResNet model\n",
    "model = dy.eval(cfg['model']['class_path'])(**cfg['model']['init_args']).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = dy.eval(\n",
    "        cfg['trainer']['optimizer']['class_path']\n",
    "    )(model.parameters(), **cfg['trainer']['optimizer']['init_args'])\n",
    "    \n",
    "try:\n",
    "    train(epoch_count=cfg['trainer']['epoch_count'])\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can easily manipulate the YAML file to their needs to have all the different experiments run. Since the configuration is generic, a lot of different setups can be run. For example, you can define your custom set of transforms, or even define your own model and optimizer.\n",
    "\n",
    "This is partially due to the dynamic nature of the [DyPy](https://github.com/vahidzee/dypy) library as well. Although not necessarily best practice, one can also define code snippets in the YAML file. This is useful for quick prototyping and testing all the possible configurations without touching the actual source code.\n",
    "\n",
    "Furthermore, we will use this configuration as a template to change it automatically using the weights and biases sweep API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Flat Sweeps to Hierarchical Sweeps\n",
    "\n",
    "Here's where we will start using the weights and biases library and later extend it with our library. First off, say we want to sweep over the following different hyper-parameters:\n",
    "1. Different learning rate values: `[0.001, 0.01]`.\n",
    "2. Different optimizers: `[SGD, Adam]`.\n",
    "3. Different epoch counts: `[10, 20]`.\n",
    "4. Different batch sizes: `[32, 64]`.\n",
    "5. For the Adam optimizer, we're going to try out different weight_decay values: `[0.1, 0.01]`.\n",
    "Note that the standard sweep configuration dictionary is very limited, and should be defined in a flat way. Therefore, we will define the sweep configuration as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer_type': {\n",
    "            'values': ['torch.optim.SGD', 'torch.optim.AdamW']\n",
    "        },\n",
    "        'optimizer_weight_decay': {\n",
    "            'values': [0.1, 0.01]  \n",
    "        },\n",
    "        'optimizer_lr': {\n",
    "            'values': [0.001, 0.01],\n",
    "        },\n",
    "        'epoch_count': {\n",
    "            'values': [10, 20],\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64],\n",
    "        },\n",
    "    } \n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # Update the cfg dictionary with sweep parameters\n",
    "        cfg = run.config\n",
    "        \n",
    "        # change the sweep configuration into the elaborate, yet generic configuration\n",
    "        with open(\"conf.yaml\", 'r') as stream:\n",
    "            base_cfg = yaml.safe_load(stream)\n",
    "        base_cfg['trainer']['optimizer']['class_path'] = cfg['optimizer_type']\n",
    "        base_cfg['trainer']['optimizer']['init_args']['lr'] = cfg['optimizer_lr']\n",
    "        if cfg['optimizer_type'] == 'torch.optim.Adam':\n",
    "            base_cfg['trainer']['optimizer']['weight_decay'] = cfg['optimizer_weight_decay']\n",
    "        base_cfg['trainer']['epoch_count'] = cfg['epoch_count']\n",
    "        base_cfg['data']['batch_size'] = cfg['batch_size']\n",
    "        \n",
    "        #... (insert your training code here) ...\n",
    "        raise NotImplementedError(\"Implement your trainer using wandb logs from the code snippet above!\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sweep configuration is flat, there should exist an extra few lines to convert the flat configuration into the generic one that was intended to work on all different running procedures. One of the many features of our library is that it eliminates that and allows W&B sweeps to also work with hierarchies.\n",
    "\n",
    "We recommend working with the `dysweep_run_resume` function that takes in all the sweep configurations as well as a function that has the following signature: `func(config, checkpoint_dir)`. For brevity, we will not explain `checkpoint_dir` here and only focus our attention on `config`. Later on, we will return to that.\n",
    "\n",
    "In this example, the `dysweep_run_resume` function will take in a base configuration as well as a sweep configuration as follows. The hierarchies also apply to the values that are being sweeped upon. Therefore, for instance, here we can see that an entire `optimizer` dictionary is being sweeped upon, in cotrast to only sweeping over primitve values.\n",
    "\n",
    "**Note**: `dysweep` uses a decoy run under the project specified that contains specific meta-data to communicate between the vanilla W&B API and our new one. Therefore, the name of the project should be specified when called; otherwise, the decoy run will be created under a different project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dysweep import dysweep_run_resume\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "\n",
    "with open(\"conf.yaml\", 'r') as stream:\n",
    "    base_cfg = yaml.safe_load(stream)\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'batch_size': {\n",
    "                'sweep': True,\n",
    "                'values': [32, 64, 128]\n",
    "            },\n",
    "        },\n",
    "        'trainer': {\n",
    "            'epoch_count': {\n",
    "                'sweep': True,\n",
    "                'values': [10, 20],\n",
    "            },\n",
    "            'optimizer': {\n",
    "                'sweep': True,\n",
    "                'sweep_alias': [\n",
    "                    'adam-lr-0.001-wd-0.1',\n",
    "                    'adam-lr-0.001-wd-0.01',\n",
    "                    'adam-lr-0.01-wd-0.1',\n",
    "                    'adam-lr-0.01-wd-0.01',\n",
    "                    'sgd-lr-0.001',\n",
    "                    'sgd-lr-0.01',\n",
    "                ],\n",
    "                'values': [\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = dysweep_run_resume(\n",
    "    project=\"tutorial\",\n",
    "    base_config=base_cfg,\n",
    "    # A hierarchical sweep configuration that is not possible with the normal sweep library\n",
    "    sweep_configuration= sweep_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dysweep_run_resume` function is multi-purpose! This means that after instantiating the sweep, you can use the same function with different arguments to run a specific configuration of that sweep. Note that you can run the following line of code **on any other machine** -- this is part of the beauty of sweep in general, that allows running configurations on different machines and even clusters for parallel computing. It should just simply have access to the same workspace and project to download all the meta-data required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dysweep import dysweep_run_resume\n",
    "from pprint import pprint\n",
    "\n",
    "# sweep_id = # Enter the sweep id obtained\n",
    "\n",
    "def train(config, checkpoint_dir):\n",
    "    ### YOUR TRAINING CODE HERE (glossing over checkpoint_dir) ###\n",
    "    print(\"Training on the following configuration:\")\n",
    "    pprint(config)\n",
    "    raise NotImplementedError(\"Implement your trainer using wandb logs from the code snippet above!\")\n",
    "\n",
    "\n",
    "ret = dysweep_run_resume(\n",
    "    project=\"tutorial\",\n",
    "    function=train,\n",
    "    count=1,\n",
    "    sweep_id=sweep_id,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Upserts\n",
    "\n",
    "Now if you take a look at the sweep configuration that was specified earlier, you will notice that you have to repeat a lot of configurations in the `optimizer` portion. In specific, for any configuration of `optimizer_type` and `weight_decay`, you would have to repeat the learning rates as well. This is not ideal, and we can use the `upsert` functionality to avoid this. The `upsert` functionality allows us to specify a list of configurations that will be merged with the base configuration. \n",
    "\n",
    "What we will do in this particular case is that we will first specify the `optimizer_type` and `weight_decay` (3 different ways). In turn, we will update the two different values for `lr` on top of that (2 different ways). Allowing us to come up with the same set of 6 different configurations, but in a much more concise way.\n",
    "\n",
    "To do so, simply change the `sweep_config` to the following:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'batch_size': {\n",
    "                'sweep': True,\n",
    "                'values': [32, 64, 128]\n",
    "            },\n",
    "        },\n",
    "        'trainer': {\n",
    "            'epoch_count': {\n",
    "                'sweep': True,\n",
    "                'values': [10, 20],\n",
    "            },\n",
    "            'optimizer': {\n",
    "                'sweep': True,\n",
    "                'sweep_alias': [\n",
    "                    'adam-wd-0.1',\n",
    "                    'adam-wd-0.01',\n",
    "                    'sgd',\n",
    "                ],\n",
    "                'values': [\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.AdamW',\n",
    "                        'init_args': {\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.AdamW',\n",
    "                        'init_args': {\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            'dy__upsert': [\n",
    "                {\n",
    "                    'sweep': True,\n",
    "                    'sweep_identifier': 'lr',\n",
    "                    'sweep_alias': [\n",
    "                        'lr-0.001',\n",
    "                        'lr-0.01',\n",
    "                    ],\n",
    "                    'values': [\n",
    "                        {\n",
    "                            'optimizer': {\n",
    "                                'init_args': {'lr': 0.001},\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            'optimizer': {\n",
    "                                'init_args': {'lr': 0.01},\n",
    "                            },\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire philosophy of dysweep revolvs around upserting configurations and we have a lot of advanced dynamic functionalities to do so. You can read more about all the ways we can alter the sweep configuration in the [documentation](https://dysweep.readthedocs.io/en/latest/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing and Re-running\n",
    "\n",
    "While doing large-scale computing, the sweep server passes on sweep configurations to the goal machine for running. If by any chance, the goal machine fails, the sweep server will not be able to re-run the same configuration again. To do so, there should be a systematic repository that saves all the run identifiers that have finished, are being run, or have failed. \n",
    "\n",
    "With dysweep, all of this is handled seamlessly for you. In fact, for every configuration, a specific checkpoint directory is specified that you can use to store model checkpoints. In turn, whenever you try to re-run, you can load the model from the checkpoint directory and continue training from there. This is the reason why the `dysweep_run_resume` function contains a `resume` keyword, and that the function passed to it can take in an extra argument `checkpoint_dir`.\n",
    "\n",
    "### Re-running\n",
    "\n",
    "Whenever dysweep instantiates a new sweep, all the logs will be saved in either `./dysweep_logs` or in a specific root directory you specify. In this directory, a set of files will be created per each of the sweeps you create. For example, if your sweep has id equal to `s6kye2ah`, then a `checkpoints-s6kye2ah` directory will appear that contains meta-data and checkpoint of all the runs associated with that sweep.\n",
    "\n",
    "Under this directory you will see two things:\n",
    "1. A set of configurations that were run successfully. These `json` files contain the entire configuration that `func` was called with and are stored in `{run_id}-config.json` format.\n",
    "2. A set of configurations that are being run currently and either have not finished, or have failed. These runs contain both configurations, and checkpoints. These information are stored in a subdirectory of the format `{i}-{run_id}`. The `i` is simply an index used internally for ordering the runs that are being run.\n",
    "Using this subdirectory, you can access specific run ids that you wish to re-run. \n",
    "\n",
    "To do so, simply pass the `rerun_id` argument in the `dysweep_run_resume` function. For example, if you wish to re-run the run with `id=xipqslhf`, you can do so as follows:\n",
    "\n",
    "```python\n",
    "dysweep_run_resume(\n",
    "    func=func,\n",
    "    resume=True,\n",
    "    count=10, # resume 10 runs\n",
    "    project='tutorial',\n",
    ")\n",
    "```\n",
    "\n",
    "### Resuming \n",
    "\n",
    "To resume a specific run, you can set the `resume` argument to `True`. Note that what this does is that whenever the function `func` is being called, the `checkpoint_dir` will be a directory that contains all of your previous checkpoints. From that point on, you can load the model from the checkpoint directory and continue training from there. \n",
    "\n",
    "### Multi-resume\n",
    "\n",
    "Say you have run a sweep for a long time and you come back to your system only to realize 10 of the runs have failed due to the machine cluster preempting the tasks. You would want to resume all of the 10 runs **using a single process** on a machine that you know will not preempt. In that case, you can call the `dysweep_run_resume` function with the `resume` option set to `True`. However, instead of defining a single `rerun_id`, you will set the `count` argument to the number of runs you wish to resume. \n",
    "\n",
    "```python\n",
    "dysweep_run_resume(\n",
    "    func=func,\n",
    "    rerun_id='xipqslhf',\n",
    "    project='tutorial',\n",
    ")\n",
    "```\n",
    "\n",
    "Using all of the information from this section you can play around with the training task to check all of these capabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Hyperparameter Search\n",
    "\n",
    "Our aim with this library is to allow for a more generic and configurable way of running experiments. Therefore, we intend on using the library for **any** large scale computing involving different configurations. \n",
    "\n",
    "For example, let's assume you want to check your model performance across a variety of datasets. Instead of running different sweeps for each dataset, we can define a standard and generic configuration scheme for all possible datasets, and then plug them into our sweep configuration. This way, we can run our model on multiple datasets using a single sweep. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
