{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Example: Image Classification\n",
    "\n",
    "This is a friendly tutorial to cover a basic image classification task. We will leverage the capabilities of `dysweep` to run different experiments on all the different configurations of the model design, as well as the dataset.\n",
    "\n",
    "First run the following piece of code to set the root directory of the project as the parent directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parent directory as the working directory\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# remove the current directory from the path\n",
    "sys.path = sys.path[1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To get started, we should first imagine the entire pipeline of our experiment. In this simple case, we have to first download the datasets for classification (in this case CIFAR10), perform any preprocessing and transforms needed to create a dataloader. Furthermore, we will also pick a specific model designed for the task at hand and train it using an optimizer. \n",
    "\n",
    "Each of the different parts of the experiments have different hyperparameters and configurations. It is good to come up with a plan of how we want to structure our configurations beforehand. A simple sketch of the run configurations as well as the simple training loop itself is given below that logs all the results in `wandb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"image_classification\")\n",
    "\n",
    "# Configurations and hyper-parameters\n",
    "\n",
    "# Training configurations\n",
    "EPOCH_COUNT = 10\n",
    "LR = 0.001\n",
    "# Data configurations\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 64\n",
    "# Model configurations\n",
    "NUM_CLASSES = 10\n",
    "PRETRAINED_OR_NOT = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define transformations for the train set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Load the ResNet model\n",
    "model = resnet50(pretrained=PRETRAINED_OR_NOT, num_classes=NUM_CLASSES).to(device) \n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training function\n",
    "def train(epoch_count=EPOCH_COUNT):\n",
    "    model.train()\n",
    "    for epoch in range(epoch_count):  # 10 epochs\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            \n",
    "        # compute the test accuracy\n",
    "        correct = 0\n",
    "        for i, data in tqdm(enumerate(test_loader, 0)):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # get the maximum logit\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # check if the prediction is correct\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        wandb.log({\"accuracy\": correct / len(test_set)})\n",
    " \n",
    "try:   \n",
    "    train()\n",
    "finally:\n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic and Configurable using Dypy\n",
    "\n",
    "To illustrate how our configurations work, we will make the above code configurable down to some of the very basic components. Usually, there is no need for such detailed configuration setup; however, we do this to showcase some features of out pipeline:\n",
    "\n",
    "1. Outline some of the functionalities of the [DyPy](https://github.com/vahidzee/dypy) library.\n",
    "2. Illustrate that the configurations can be complex and hierarchical, but generic in a sense that you can manipulate everything down to the very last detail using a configuration file.\n",
    "\n",
    "Furthermore, we will define a YAML configuration file that will allow us to define the entire experiment, and that these experiments are tunable down to the very last details. The following would be the dictionary that is stored in the yaml file that can accessed [here](./conf.yaml):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'data': {\n",
    "        \"dataset_class\": \"torchvision.datasets.CIFAR10\",\n",
    "        \"batch_size\": 64,\n",
    "        \"num_workers\": 2,\n",
    "        \"train_transforms\": [\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.RandomHorizontalFlip\",\n",
    "            },\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.RandomCrop\",\n",
    "                \"init_args\": {\n",
    "                    \"size\": 32,\n",
    "                    \"padding\": 4\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.ToTensor\",\n",
    "            },\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                \"init_args\": {\n",
    "                    \"mean\": [0.4914, 0.4822, 0.4465],\n",
    "                    \"std\": [0.2023, 0.1994, 0.2010]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"test_transforms\": [\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.ToTensor\",\n",
    "            },\n",
    "            {\n",
    "                \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                \"init_args\": {\n",
    "                    \"mean\": [0.4914, 0.4822, 0.4465],\n",
    "                    \"std\": [0.2023, 0.1994, 0.2010]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"class_path\": \"torchvision.models.resnet50\",\n",
    "        \"init_args\": {\n",
    "            \"pretrained\": False,\n",
    "            \"num_classes\": 10\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"epoch_count\": 10,\n",
    "        \"optimizer\": {\n",
    "            \"class_path\": \"torch.optim.SGD\",\n",
    "            \"init_args\": {\n",
    "                \"lr\": 0.001\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we assume `config` is the above configuration, we can re-write the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from tqdm import tqdm\n",
    "import dypy as dy\n",
    "import yaml\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load conf.yaml into the dictionary config\n",
    "with open(\"conf.yaml\", 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "\n",
    "# Define transformations for the train set\n",
    "train_transform = transforms.Compose([\n",
    "    dy.eval(x['class_path'])(**(x['init_args'] if 'init_args' in x else {})) for \n",
    "        x in cfg['data']['train_transforms']\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "test_transform = transforms.Compose([\n",
    "    dy.eval(x['class_path'])(**(x['init_args'] if 'init_args' in x else {})) for \n",
    "        x in cfg['data']['test_transforms']\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = dy.eval(cfg['data']['dataset_class'])(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_set = dy.eval(cfg['data']['dataset_class'])(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=cfg['data']['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=cfg['data']['num_workers'],\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=cfg['data']['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=cfg['data']['num_workers'],\n",
    ")\n",
    "\n",
    "# Load the ResNet model\n",
    "model = dy.eval(cfg['model']['class_path'])(**cfg['model']['init_args']).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = dy.eval(\n",
    "        cfg['trainer']['optimizer']['class_path']\n",
    "    )(model.parameters(), **cfg['trainer']['optimizer']['init_args'])\n",
    "    \n",
    "try:\n",
    "    train(epoch_count=cfg['trainer']['epoch_count'])\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can easily manipulate the YAML file to their needs to have all the different experiments run. Since the configuration is generic, a lot of different setups can be run. For example, you can define your custom set of transforms, or even define your own model and optimizer.\n",
    "\n",
    "This is partially due to the dynamic nature of the [DyPy](https://github.com/vahidzee/dypy) library as well. Although not necessarily best practice, one can also define code snippets in the YAML file. This is useful for quick prototyping and testing all the possible configurations without touching the actual source code.\n",
    "\n",
    "Furthermore, we will use this configuration as a template to change it automatically using the weights and biases sweep API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Flat Sweeps to Hierarchical Sweeps\n",
    "\n",
    "Here's where we will start using the weights and biases library and later extend it with our library. First off, say we want to sweep over the following different hyper-parameters:\n",
    "1. Different learning rate values: `[0.001, 0.01]`.\n",
    "2. Different optimizers: `[SGD, Adam]`.\n",
    "3. Different epoch counts: `[10, 20]`.\n",
    "4. Different batch sizes: `[32, 64]`.\n",
    "5. For the Adam optimizer, we're going to try out different weight_decay values: `[0.1, 0.01]`.\n",
    "Note that the standard sweep configuration dictionary is very limited, and should be defined in a flat way. Therefore, we will define the sweep configuration as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer_type': {\n",
    "            'values': ['torch.optim.SGD', 'torch.optim.AdamW']\n",
    "        },\n",
    "        'optimizer_weight_decay': {\n",
    "            'values': [0.1, 0.01]  \n",
    "        },\n",
    "        'optimizer_lr': {\n",
    "            'values': [0.001, 0.01],\n",
    "        },\n",
    "        'epoch_count': {\n",
    "            'values': [10, 20],\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64],\n",
    "        },\n",
    "    } \n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # Update the cfg dictionary with sweep parameters\n",
    "        cfg = run.config\n",
    "        \n",
    "        # change the sweep configuration into the elaborate, yet generic configuration\n",
    "        with open(\"conf.yaml\", 'r') as stream:\n",
    "            base_cfg = yaml.safe_load(stream)\n",
    "        base_cfg['trainer']['optimizer']['class_path'] = cfg['optimizer_type']\n",
    "        base_cfg['trainer']['optimizer']['init_args']['lr'] = cfg['optimizer_lr']\n",
    "        if cfg['optimizer_type'] == 'torch.optim.Adam':\n",
    "            base_cfg['trainer']['optimizer']['weight_decay'] = cfg['optimizer_weight_decay']\n",
    "        base_cfg['trainer']['epoch_count'] = cfg['epoch_count']\n",
    "        base_cfg['data']['batch_size'] = cfg['batch_size']\n",
    "        \n",
    "        #... (insert your training code here) ...\n",
    "        raise NotImplementedError(\"Implement your trainer using wandb logs from the code snippet above!\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sweep configuration is flat, there should exist an extra few lines to convert the flat configuration into the generic one that was intended to work on all different running procedures. One of the many features of our library is that it eliminates that and allows W&B sweeps to also work with hierarchies.\n",
    "\n",
    "We recommend working with the `dysweep_run_resume` function that takes in all the sweep configurations as well as a function that has the following signature: `func(config, checkpoint_dir)`. For brevity, we will not explain `checkpoint_dir` here and only focus our attention on `config`. Later on, we will return to that.\n",
    "\n",
    "In this example, the `dysweep_run_resume` function will take in a base configuration as well as a sweep configuration as follows. The hierarchies also apply to the values that are being sweeped upon. Therefore, for instance, here we can see that an entire `optimizer` dictionary is being sweeped upon, in cotrast to only sweeping over primitve values.\n",
    "\n",
    "**Note**: `dysweep` uses a decoy run under the project specified that contains specific meta-data to communicate between the vanilla W&B API and our new one. Therefore, the name of the project should be specified when called; otherwise, the decoy run will be created under a different project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dysweep import dysweep_run_resume\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "\n",
    "with open(\"conf.yaml\", 'r') as stream:\n",
    "    base_cfg = yaml.safe_load(stream)\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'batch_size': {\n",
    "                'sweep': True,\n",
    "                'values': [32, 64, 128]\n",
    "            },\n",
    "        },\n",
    "        'trainer': {\n",
    "            'epoch_count': {\n",
    "                'sweep': True,\n",
    "                'values': [10, 20],\n",
    "            },\n",
    "            'optimizer': {\n",
    "                'sweep': True,\n",
    "                'sweep_alias': [\n",
    "                    'adam-lr-0.001-wd-0.1',\n",
    "                    'adam-lr-0.001-wd-0.01',\n",
    "                    'adam-lr-0.01-wd-0.1',\n",
    "                    'adam-lr-0.01-wd-0.01',\n",
    "                    'sgd-lr-0.001',\n",
    "                    'sgd-lr-0.01',\n",
    "                ],\n",
    "                'values': [\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.Adam',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.001,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                        'init_args': {\n",
    "                            'lr': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = dysweep_run_resume(\n",
    "    project=\"tutorial\",\n",
    "    base_config=base_cfg,\n",
    "    # A hierarchical sweep configuration that is not possible with the normal sweep library\n",
    "    sweep_configuration= sweep_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dysweep_run_resume` function is multi-purpose! This means that after instantiating the sweep, you can use the same function with different arguments to run a specific configuration of that sweep. Note that you can run the following line of code **on any other machine** -- this is part of the beauty of sweep in general, that allows running configurations on different machines and even clusters for parallel computing. It should just simply have access to the same workspace and project to download all the meta-data required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dysweep import dysweep_run_resume\n",
    "from pprint import pprint\n",
    "\n",
    "# sweep_id = # Enter the sweep id obtained\n",
    "\n",
    "def train(config, checkpoint_dir):\n",
    "    ### YOUR TRAINING CODE HERE (glossing over checkpoint_dir) ###\n",
    "    print(\"Training on the following configuration:\")\n",
    "    pprint(config)\n",
    "    raise NotImplementedError(\"Implement your trainer using wandb logs from the code snippet above!\")\n",
    "\n",
    "\n",
    "ret = dysweep_run_resume(\n",
    "    project=\"tutorial\",\n",
    "    function=train,\n",
    "    count=1,\n",
    "    sweep_id=sweep_id,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Upserts\n",
    "\n",
    "Now if you take a look at the sweep configuration that was specified earlier, you will notice that you have to repeat a lot of configurations in the `optimizer` portion. In specific, for any configuration of `optimizer_type` and `weight_decay`, you would have to repeat the learning rates as well. This is not ideal, and we can use the `upsert` functionality to avoid this. The `upsert` functionality allows us to specify a list of configurations that will be merged with the base configuration. \n",
    "\n",
    "What we will do in this particular case is that we will first specify the `optimizer_type` and `weight_decay` (3 different ways). In turn, we will update the two different values for `lr` on top of that (2 different ways). Allowing us to come up with the same set of 6 different configurations, but in a much more concise way.\n",
    "\n",
    "To do so, simply change the `sweep_config` to the following:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'my-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'batch_size': {\n",
    "                'sweep': True,\n",
    "                'values': [32, 64, 128]\n",
    "            },\n",
    "        },\n",
    "        'trainer': {\n",
    "            'epoch_count': {\n",
    "                'sweep': True,\n",
    "                'values': [10, 20],\n",
    "            },\n",
    "            'optimizer': {\n",
    "                'sweep': True,\n",
    "                'sweep_alias': [\n",
    "                    'adam-wd-0.1',\n",
    "                    'adam-wd-0.01',\n",
    "                    'sgd',\n",
    "                ],\n",
    "                'values': [\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.AdamW',\n",
    "                        'init_args': {\n",
    "                            'weight_decay': 0.1,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.AdamW',\n",
    "                        'init_args': {\n",
    "                            'weight_decay': 0.01,\n",
    "                        },  \n",
    "                    },\n",
    "                    {\n",
    "                        'class_path': 'torch.optim.SGD',\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            'dy__upsert': [\n",
    "                {\n",
    "                    'sweep': True,\n",
    "                    'sweep_identifier': 'lr',\n",
    "                    'sweep_alias': [\n",
    "                        'lr-0.001',\n",
    "                        'lr-0.01',\n",
    "                    ],\n",
    "                    'values': [\n",
    "                        {\n",
    "                            'optimizer': {\n",
    "                                'init_args': {'lr': 0.001},\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            'optimizer': {\n",
    "                                'init_args': {'lr': 0.01},\n",
    "                            },\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire philosophy of dysweep revolvs around upserting configurations and we have a lot of advanced dynamic functionalities to do so. You can read more about all the ways we can alter the sweep configuration in the [documentation](https://dysweep.readthedocs.io/en/latest/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing and Re-running\n",
    "\n",
    "While doing large-scale computing, the sweep server passes on sweep configurations to the goal machine for running. If by any chance, the goal machine fails, the sweep server will not be able to re-run the same configuration again. To do so, there should be a systematic repository that saves all the run identifiers that have finished, are being run, or have failed. \n",
    "\n",
    "With dysweep, all of this is handled seamlessly for you. In fact, for every configuration, a specific checkpoint directory is specified that you can use to store model checkpoints. In turn, whenever you try to re-run, you can load the model from the checkpoint directory and continue training from there. This is the reason why the `dysweep_run_resume` function contains a `resume` keyword, and that the function passed to it can take in an extra argument `checkpoint_dir`.\n",
    "\n",
    "### Re-running\n",
    "\n",
    "Whenever dysweep instantiates a new sweep, all the logs will be saved in either `./dysweep_logs` or in a specific root directory you specify. In this directory, a set of files will be created per each of the sweeps you create. For example, if your sweep has id equal to `s6kye2ah`, then a `checkpoints-s6kye2ah` directory will appear that contains meta-data and checkpoint of all the runs associated with that sweep.\n",
    "\n",
    "Under this directory you will see two things:\n",
    "1. A set of configurations that were run successfully. These `json` files contain the entire configuration that `func` was called with and are stored in `{run_id}-config.json` format.\n",
    "2. A set of configurations that are being run currently and either have not finished, or have failed. These runs contain both configurations, and checkpoints. These information are stored in a subdirectory of the format `{i}-{run_id}`. The `i` is simply an index used internally for ordering the runs that are being run.\n",
    "Using this subdirectory, you can access specific run ids that you wish to re-run. \n",
    "\n",
    "To do so, simply pass the `rerun_id` argument in the `dysweep_run_resume` function. For example, if you wish to re-run the run with `id=xipqslhf`, you can do so as follows:\n",
    "```python\n",
    "dysweep_run_resume(\n",
    "    func=func,\n",
    "    rerun_id='xipqslhf',\n",
    "    project='tutorial',\n",
    ")\n",
    "```\n",
    "\n",
    "### Resuming \n",
    "\n",
    "To resume a specific run, you can set the `resume` argument to `True`. Note that what this does is that whenever the function `func` is being called, the `checkpoint_dir` will be a directory that contains all of your previous checkpoints. From that point on, you can load the model from the checkpoint directory and continue training from there. \n",
    "\n",
    "### Multi-resume\n",
    "\n",
    "Say you have run a sweep for a long time and you come back to your system only to realize 10 of the runs have failed due to the machine cluster preempting the tasks. You would want to resume all of the 10 runs **using a single process** on a machine that you know will not preempt. In that case, you can call the `dysweep_run_resume` function with the `resume` option set to `True`. However, instead of defining a single `rerun_id`, you will set the `count` argument to the number of runs you wish to resume. \n",
    "\n",
    "```python\n",
    "dysweep_run_resume(\n",
    "    func=func,\n",
    "    resume=True,\n",
    "    count=10, # resume 10 runs\n",
    "    project='tutorial',\n",
    ")\n",
    "```\n",
    "\n",
    "Using all of the information from this section you can play around with the training task to check all of these capabilities. In particular, we have implemented the full task in two source codes. The one for [instantiating](../testing/main_sweep_maker1.py) the sweep and the one for [running](../testing/main_sweep_user.py) the sweep.\n",
    "\n",
    "To do so, run the following command:\n",
    "\n",
    "```bash\n",
    "cd ../testing/\n",
    "python main_sweep_runner.py\n",
    "```\n",
    "\n",
    "Then, you can access the sweep id by simply looking at the output of the command. Let's denote the output by `ssssssss`. Then, you can run the following command to run the sweep:\n",
    "\n",
    "```bash\n",
    "python main_sweep_user.py --sweep_id=ssssssss --count=<the-number-of-runs-to-trigger-with-the-process>\n",
    "```\n",
    "\n",
    "You can also interrupt any of the processes and take a look at the `dysweep_logs` directory to see how the logs are being saved. Under each `checkpoings-<sweep_id>` directory, you will see the configurations that have successfully ran (in the form of `<run-id>-config.json`), plus, a set of directories of the runs that are either running at the moment or have been killed or interrupted (in the form of `<i>-<run-id>`).\n",
    "\n",
    "Now, you can re-run any of the runs that have logs in the `dysweep_logs` directory. To do so, simply check the run identifier of your desired run (let's assume it is `rrrrrrrr`) and run the following command:\n",
    "\n",
    "```bash\n",
    "python main_sweep_user.py --sweep_id=ssssssss --rerun_id=rrrrrrrr --resume=<True/False>\n",
    "```\n",
    "\n",
    "With the `resume` knob you can check whether you want to re-run or resume. Note that the code implemented in the testing section follows the `jsonargparse` standard which we descibe in the documentations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Hyperparameter Search\n",
    "\n",
    "Our aim with this library is to allow for a more generic and configurable way of running experiments. Therefore, we intend on using the library for **any** large scale computing involving different configurations that can be **generated in a systematic way**.\n",
    "\n",
    "For example, let's assume you want to check your model performance across a variety of datasets or check what is the effect of normalization on the input. Instead of running different sweeps for each dataset, we can define a standard and generic configuration scheme for all possible datasets, and then plug them into our sweep configuration. This way, we can run our model on multiple datasets using a single sweep. \n",
    "\n",
    "Moreover, as a special use-case in research, after developing an optimized model, we want to benchmark it against different baselines on different datasets. All of these different configurations can be systematically sweeped upon using dysweep."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Effect of Normalization -- A Use-Case of List Operations\n",
    "\n",
    "Assume you want to check how much normalization affects the performance of the model. If we take a look at the configuration used for this problem, we can see that the training and testing transforms contain a list of different transformations where the last transformation normalizes w.r.t the mean and standard deviation of the training set. \n",
    "\n",
    "Using dysweep upsert, you can also define operations over the configuration. Note that the philosophy behind dysweep is to start off with the base configuration, and then apply different changes to the configuration. One such change would be to **remove** the last element of the lists `train_transforms` and `test_transforms`. This way, we can run the model without normalization.\n",
    "\n",
    "The following sweep configuration, will do exactly that:\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'normalization-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'dy__upsert': [\n",
    "                {\n",
    "                    'sweep': True,\n",
    "                    'sweep_identifier': 'norm_no_norm',\n",
    "                    'sweep_alias': ['without_norm', 'with_norm'],\n",
    "                    'values': [\n",
    "                        # remove the last transform from both train and test transforms\n",
    "                        {\n",
    "                            'train_transforms': {\n",
    "                                'dy__list__operations': [\n",
    "                                    {'dy__remove': -1}\n",
    "                                ]\n",
    "                            },\n",
    "                            'test_transforms': {\n",
    "                                'dy__list__operations': [\n",
    "                                    {'dy__remove': -1}\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        # leave as is\n",
    "                        {}\n",
    "                    ]   \n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You can also do the same thing without list operations, but it would require a hefty configuration where you define both lists `train_transforms` and `test_transforms` for each of the two cases. We have implemented the sweep instantiator in [this](../testing/sweep_maker2.py) source code that you can run similar to before.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Checking the Model on Different Datasets\n",
    "\n",
    "To demonstrate the capability of our package to move beyond only hyper-parameter tuning, we have also devised another sweep configuration to showcase that we can also use dysweep for running experiments on different datasets. To do so, we have also considered the classification task of [CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html). For changing datasets, we only need to change the transforms and dataset class, as well as the number of classes in our model logits output. This can be done using the following:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'dataset-sweep',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'dy__upsert': [\n",
    "            {\n",
    "                'sweep': False,\n",
    "                'sweep_identifier': 'dataset',\n",
    "                'sweep_alias': ['CIFAR100', 'CIFAR10'],\n",
    "                'values': [\n",
    "                    # CIFAR100\n",
    "                    {\n",
    "                        'data': {\n",
    "                            'dataset_class': 'torchvision.datasets.CIFAR100',\n",
    "                            'train_transforms': {\n",
    "                                'dy__list__operations': [\n",
    "                                    {\n",
    "                                        'dy__overwrite': [\n",
    "                                            3,\n",
    "                                            {\n",
    "                                                \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                                                \"init_args\": {\n",
    "                                                    \"mean\": [0.5071, 0.4865, 0.4409],\n",
    "                                                    \"std\": [0.2673, 0.2564, 0.2762]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    },\n",
    "                                ]\n",
    "                            },\n",
    "                            'test_transforms': {\n",
    "                                'dy__list__operations': [\n",
    "                                    {\n",
    "                                        'dy__overwrite': [\n",
    "                                            1,\n",
    "                                            {\n",
    "                                                \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                                                \"init_args\": {\n",
    "                                                    \"mean\": [0.5071, 0.4865, 0.4409],\n",
    "                                                    \"std\": [0.2673, 0.2564, 0.2762]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    },\n",
    "                                ]\n",
    "                            },\n",
    "                        },\n",
    "                        'model': {\n",
    "                            'init_args': {\n",
    "                                'num_classes': 100\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    # leave as is \n",
    "                    {}\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use-Case of Linking using `dy__eval` (Advanced Linking)\n",
    "\n",
    "While [Pyyaml](https://github.com/yaml/pyyaml/) provides dynamic linking capabilites, [Dypy](https://github.com/vahidzee/dypy) takes it to the next level by introducing code-snippets in the YAML configuration. We leverage this capability of Dypy to introduce a new linking mechanism. As a use-case, we will implement the same sweep configuration as before, but this time we will use `dy__eval` instead of using list operations.\n",
    "\n",
    "For any particular attribute in the configurations, you can use `dy__eval` to set that attribute. `dy__eval` can be set to a code snippet of a function that takes in the **current configuration**, and using that, returns the value of the attribute of interest. \n",
    "\n",
    "For example, here, we may only sweep over the attribute `dataset_class` and then according to the value it has been set to, we can define the transform normalization values as well as the model class count. To do so, we can run the following configuration:\n",
    "\n",
    "```python\n",
    "get_transform_from_conf = \"\"\"\n",
    "def func(conf):\n",
    "    dataset_type = conf['data']['dataset_class'].split('.')[-1]\n",
    "    if dataset_type == 'CIFAR100':\n",
    "        return {\n",
    "            'mean': [0.5071, 0.4865, 0.4409],\n",
    "            'std': [0.2673, 0.2564, 0.2762]\n",
    "        }\n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        return {\n",
    "            'mean': [0.4914, 0.4822, 0.4465],\n",
    "            'std': [0.2023, 0.1994, 0.2010]\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "get_num_classes_from_conf = \"\"\"\n",
    "def func(conf):\n",
    "    dataset_type = conf['data']['dataset_class'].split('.')[-1]\n",
    "    if dataset_type == 'CIFAR100':\n",
    "        return 100\n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        return 10\n",
    "\"\"\"\n",
    "\n",
    "{\n",
    "    'name': 'dataset-sweep-dy-eval',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data': {\n",
    "            'dataset_class': {\n",
    "                'sweep': True,\n",
    "                'values': [\n",
    "                    'torchvision.datasets.CIFAR100',\n",
    "                    'torchvision.datasets.CIFAR10'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        'dy__upsert': [\n",
    "            {\n",
    "                'data': {\n",
    "                    \"train_transforms\": [\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.RandomHorizontalFlip\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.RandomCrop\",\n",
    "                            \"init_args\": {\n",
    "                                \"size\": 32,\n",
    "                                \"padding\": 4\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.ToTensor\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                            # set the transform according to the dataset dynamically\n",
    "                            \"init_args\": {\n",
    "                                \"dy__eval\": {\n",
    "                                    \"expression\": get_transform_from_conf,\n",
    "                                    \"function_of_interest\": \"func\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"test_transforms\": [\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.ToTensor\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"class_path\": \"torchvision.transforms.Normalize\",\n",
    "                            # set the transform according to the dataset dynamically\n",
    "                            \"init_args\": {\n",
    "                                \"dy__eval\": {\n",
    "                                    \"expression\": get_transform_from_conf,\n",
    "                                    \"function_of_interest\": \"func\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                'model' : {\n",
    "                    'init_args': {\n",
    "                        'num_classes': {\n",
    "                            'dy__eval': {\n",
    "                                'expression': get_num_classes_from_conf,\n",
    "                                'function_of_interest': 'func',\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "You can also run this configuration by running the third sweep maker [script](../testing/sweep_maker4.py). For more detail on `dy__eval` please check out our documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
